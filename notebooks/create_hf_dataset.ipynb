{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datasets import load_dataset\n",
    "\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from copy import copy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    FlaxAutoModelForSequenceClassification,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    TrainingArguments,\n",
    "    is_tensorboard_available,\n",
    ")\n",
    "\n",
    "from flax.training.common_utils import get_metrics, onehot, shard\n",
    "\n",
    "\n",
    "data_root = \"/kaggle/input/feedback-prize-effectiveness/\"\n",
    "train = pd.read_csv(\"/kaggle/input/feedback-prize-effectiveness/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}\n",
    "\n",
    "def _prepare_training_data_helper(args, tokenizer, df, is_train):\n",
    "    training_samples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        idx = row[\"essay_id\"]\n",
    "        discourse_text = row[\"discourse_text\"]\n",
    "        discourse_type = row[\"discourse_type\"]\n",
    "\n",
    "        if is_train:\n",
    "            filename = os.path.join(args.input, \"train\", idx + \".txt\")\n",
    "        else:\n",
    "            filename = os.path.join(args.input, \"test\", idx + \".txt\")\n",
    "\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        sep_token = tokenizer.sep_token\n",
    "\n",
    "        # encoded_text = tokenizer.encode_plus(\n",
    "        #     discourse_type + sep_token + discourse_text + sep_token + text,\n",
    "        #     add_special_tokens=False,\n",
    "        #     padding=\"max_length\",\n",
    "        #     truncation=True,\n",
    "        #     max_length=512 ##TODO: update max_length\n",
    "        # )\n",
    "\n",
    "        # encoded_text = tokenizer.encode_plus(\n",
    "        #     discourse_type + \" \" + discourse_text,\n",
    "        #     text,\n",
    "        #     add_special_tokens=False,\n",
    "        #     padding=\"max_length\",\n",
    "        #     truncation=True,\n",
    "        #     max_length=512 ##TODO: update max_length\n",
    "        # )\n",
    "\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            discourse_type.lower() + sep_token + discourse_text.lower(),\n",
    "            text.lower(),\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512 ##TODO: update max_length\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "\n",
    "        sample = {\n",
    "            # \"discourse_id\": row[\"discourse_id\"],\n",
    "            \"input_ids\": input_ids,\n",
    "            # \"discourse_text\": discourse_text,\n",
    "            # \"essay_text\": text,\n",
    "            \"attention_mask\": encoded_text[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "        if \"token_type_ids\" in encoded_text:\n",
    "            sample[\"token_type_ids\"] = encoded_text[\"token_type_ids\"]\n",
    "\n",
    "        try:\n",
    "            label = row[\"discourse_effectiveness\"]\n",
    "            sample[\"labels\"] = LABEL_MAPPING[label]\n",
    "        except:\n",
    "            sample[\"labels\"] = 0\n",
    "        \n",
    "\n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "def prepare_training_data(df, tokenizer, args, num_jobs, is_train):\n",
    "    training_samples = []\n",
    "\n",
    "    df_splits = np.array_split(df, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_training_data_helper)(args, tokenizer, df, is_train) for df in df_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        training_samples.extend(result)\n",
    "\n",
    "    return training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../configs\")\n",
    "cfg = copy(importlib.import_module(\"elu_config\").cfg)\n",
    "\n",
    "print(cfg.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-base'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "config = AutoConfig.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    num_labels=cfg.num_labels,\n",
    "    #finetuning_task=data_args.task_name,\n",
    "    #use_auth_token=True if cfg.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    use_fast=not cfg.use_slow_tokenizer,\n",
    "    #use_auth_token=True if cfg.use_auth_token else None,\n",
    ")\n",
    "# model = FlaxAutoModelForSequenceClassification.from_pretrained(\n",
    "#     # cfg.model_name_or_path,\n",
    "#     cfg.model_name_or_path,\n",
    "#     config=config,\n",
    "#     ignore_mismatched_sizes=True,\n",
    "#     #use_auth_token=True if cfg.use_auth_token else None,\n",
    "# )\n",
    "\n",
    "cfg.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "# kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "# ## stratified Kfold for train dataframe using discourse_type and discourse_effectiveness\n",
    "\n",
    "# for fold, (train_index, valid_index) in enumerate(kf.split(train, train[\"discourse_type\"], train[\"discourse_effectiveness\"])):\n",
    "# # for fold, (train_index, valid_index) in enumerate(kf.split(train, train[\"discourse_effectiveness\"], train[\"essay_id\"])):\n",
    "\n",
    "#     train_temp = train.iloc[train_index]\n",
    "#     valid_temp = train.iloc[valid_index]\n",
    "\n",
    "#     train_data = prepare_training_data(train_temp, tokenizer, cfg, num_jobs=96, is_train=True)\n",
    "#     val_data = prepare_training_data(valid_temp, tokenizer, cfg, num_jobs=96, is_train=True)\n",
    "\n",
    "#     df = pd.DataFrame.from_records(train_data)\n",
    "#     df.to_json(f\"/kaggle/working/folds/train_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "#     df = pd.DataFrame.from_records(val_data)\n",
    "#     df.to_json(f\"/kaggle/working/folds/valid_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "#     clear_output()\n",
    "#     break\n",
    "#     # print(\"Fold:\", fold)\n",
    "#     # print(\"Train:\", train_index)\n",
    "#     # print(\"Valid:\", valid_index)\n",
    "#     # print(\"\\n\")\n",
    "\n",
    "gkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(gkf.split(train, train[\"discourse_effectiveness\"], train[\"essay_id\"])):\n",
    "    train_temp = train.iloc[train_index]\n",
    "    valid_temp = train.iloc[valid_index]\n",
    "\n",
    "    train_data = prepare_training_data(train_temp, tokenizer, cfg, num_jobs=96, is_train=True)\n",
    "    val_data = prepare_training_data(valid_temp, tokenizer, cfg, num_jobs=96, is_train=True)\n",
    "\n",
    "    df = pd.DataFrame.from_records(train_data)\n",
    "    df.to_json(f\"/kaggle/working/folds/train_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    df = pd.DataFrame.from_records(val_data)\n",
    "    df.to_json(f\"/kaggle/working/folds/valid_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "    clear_output()\n",
    "\n",
    "    # print(\"Fold:\", fold)\n",
    "    # print(\"Train:\", train_index)\n",
    "    # print(\"Valid:\", valid_index)\n",
    "    # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d(train_temp.essay_id, valid_temp.essay_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## generate test dataset\n",
    "# test = pd.read_csv(\"/kaggle/input/feedback-prize-effectiveness/test.csv\")\n",
    "# test_data = prepare_training_data(test, tokenizer, cfg, num_jobs=96, is_train=False)\n",
    "# df = pd.DataFrame.from_records(test_data)\n",
    "# df.to_json(f\"/kaggle/working/folds/test.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "## data collator with dynamic padding\n",
    "# def train_data_collator(rng:)\n",
    "import jax\n",
    "import datasets\n",
    "from typing import Any, Callable, Dict, Optional, Tuple\n",
    "\n",
    "rng = jax.random.PRNGKey(1)#cfg.seed)\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "Array = Any\n",
    "Dataset = datasets.arrow_dataset.Dataset\n",
    "PRNGKey = Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-330ccf216b3e996e\n",
      "WARNING:datasets.builder:Reusing dataset json (/root/.cache/huggingface/datasets/json/default-330ccf216b3e996e/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n",
      "WARNING:datasets.builder:Using custom data configuration default-b7a86f930b8e2869\n",
      "WARNING:datasets.builder:Reusing dataset json (/root/.cache/huggingface/datasets/json/default-b7a86f930b8e2869/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<s> claim</s> i think that the face is a natural landform because there is no life on mars that we have descovered yet</s></s> hi, i\\'m isaac, i\\'m going to be writing about how this face on mars is a natural landform or if there is life on mars that made it. the story is about how nasa took a picture of mars and a face was seen on the planet. nasa doesn\\'t know if the landform was created by life on mars, or if it is just a natural landform. on my perspective, i think that the face is a natural landform because i dont think that there is any life on mars. in these next few paragraphs, i\\'ll be talking about how i think that is is a natural landform i think that the face is a natural landform because there is no life on mars that we have descovered yet. if life was on mars, we would know by now. the reason why i think it is a natural landform because, nobody live on mars in order to create the figure. it says in paragraph 9, \"it\\'s not easy to target cydonia,\" in which he is saying that its not easy to know if it is a natural landform at this point. in all that they\\'re saying, its probably a natural landform. people thought that the face was formed by alieans because they thought that there was life on mars. though some say that life on mars does exist, i think that there is no life on mars. it says in paragraph 7, on april 5, 1998, mars global surveyor flew over cydonia for the first time. michael malin took a picture of mars with his orbiter camera, that the face was a natural landform. everyone who thought it was made by alieans even though it wasn\\'t, was not satisfied. i think they were not satisfied because they have thought since 1976 that it was really formed by alieans. though people were not satified about how the landform was a natural landform, in all, we new that alieans did not form the face. i would like to know how the landform was formed. we know now that life on mars doesn\\'t exist.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n"
     ]
    }
   ],
   "source": [
    "def eval_data_collator(dataset: Dataset, batch_size: int):\n",
    "    \"\"\"Returns batches of size `batch_size` from `eval dataset`, sharded over all local devices.\"\"\"\n",
    "    for i in range(len(dataset) // batch_size):\n",
    "        batch = dataset[i * batch_size : (i + 1) * batch_size]\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "\n",
    "        yield batch\n",
    "\n",
    "rng = jax.random.PRNGKey(cfg.seed)\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
    "rng, input_rng = jax.random.split(rng)\n",
    "eval_dataset   = load_dataset(\"json\", data_files=f\"/kaggle/working/folds/valid_{cfg.fold}.jsonl\", split=\"train\")\n",
    "eval_loader = eval_data_collator(eval_dataset, 4)\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=f\"/kaggle/working/folds/train_{cfg.fold}.jsonl\", split=\"train\")\n",
    "# train_loader = train_data_collator(train_dataset, 4)\n",
    "\n",
    "## decode text using tokenizer\n",
    "text = eval_dataset[0]['input_ids']\n",
    "text = tokenizer.decode(text)\n",
    "print(repr(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4021581/887645117.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batch = {k: np.array(v) for k, v in batch.items()}\n"
     ]
    }
   ],
   "source": [
    "for batch in eval_loader:\n",
    "    print(batch['input_ids'].shape)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.nonzero(train_dataset[0]['input_ids'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22171,\n",
       " 415,\n",
       " 993,\n",
       " 427,\n",
       " 363,\n",
       " 2087,\n",
       " 419,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 881,\n",
       " 713,\n",
       " 419,\n",
       " 746,\n",
       " 1305,\n",
       " 420,\n",
       " 8807,\n",
       " 427,\n",
       " 457,\n",
       " 524,\n",
       " 849,\n",
       " 32212,\n",
       " 1966,\n",
       " 321,\n",
       " 16003,\n",
       " 112,\n",
       " 1413,\n",
       " 1202,\n",
       " 19169,\n",
       " 112,\n",
       " 1413,\n",
       " 1202,\n",
       " 1117,\n",
       " 385,\n",
       " 408,\n",
       " 3698,\n",
       " 647,\n",
       " 804,\n",
       " 529,\n",
       " 2087,\n",
       " 420,\n",
       " 8807,\n",
       " 419,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 494,\n",
       " 712,\n",
       " 713,\n",
       " 419,\n",
       " 1305,\n",
       " 420,\n",
       " 8807,\n",
       " 427,\n",
       " 1026,\n",
       " 441,\n",
       " 114,\n",
       " 484,\n",
       " 1722,\n",
       " 419,\n",
       " 647,\n",
       " 804,\n",
       " 8985,\n",
       " 1819,\n",
       " 358,\n",
       " 4387,\n",
       " 387,\n",
       " 8807,\n",
       " 391,\n",
       " 358,\n",
       " 2087,\n",
       " 474,\n",
       " 1876,\n",
       " 420,\n",
       " 363,\n",
       " 5541,\n",
       " 114,\n",
       " 8985,\n",
       " 1696,\n",
       " 571,\n",
       " 861,\n",
       " 712,\n",
       " 363,\n",
       " 2057,\n",
       " 788,\n",
       " 474,\n",
       " 2828,\n",
       " 517,\n",
       " 1305,\n",
       " 420,\n",
       " 8807,\n",
       " 112,\n",
       " 494,\n",
       " 712,\n",
       " 441,\n",
       " 419,\n",
       " 756,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 114,\n",
       " 1651,\n",
       " 717,\n",
       " 6751,\n",
       " 112,\n",
       " 415,\n",
       " 993,\n",
       " 427,\n",
       " 363,\n",
       " 2087,\n",
       " 419,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 881,\n",
       " 415,\n",
       " 17767,\n",
       " 993,\n",
       " 427,\n",
       " 713,\n",
       " 419,\n",
       " 698,\n",
       " 1305,\n",
       " 420,\n",
       " 8807,\n",
       " 114,\n",
       " 655,\n",
       " 878,\n",
       " 1407,\n",
       " 1279,\n",
       " 23650,\n",
       " 112,\n",
       " 415,\n",
       " 1284,\n",
       " 408,\n",
       " 3476,\n",
       " 647,\n",
       " 804,\n",
       " 415,\n",
       " 993,\n",
       " 427,\n",
       " 419,\n",
       " 419,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 100,\n",
       " 141,\n",
       " 993,\n",
       " 427,\n",
       " 363,\n",
       " 2087,\n",
       " 419,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 881,\n",
       " 713,\n",
       " 419,\n",
       " 746,\n",
       " 1305,\n",
       " 420,\n",
       " 8807,\n",
       " 427,\n",
       " 457,\n",
       " 524,\n",
       " 849,\n",
       " 32212,\n",
       " 1966,\n",
       " 114,\n",
       " 1103,\n",
       " 1305,\n",
       " 474,\n",
       " 420,\n",
       " 8807,\n",
       " 112,\n",
       " 457,\n",
       " 662,\n",
       " 861,\n",
       " 517,\n",
       " 884,\n",
       " 114,\n",
       " 484,\n",
       " 1839,\n",
       " 1622,\n",
       " 415,\n",
       " 993,\n",
       " 441,\n",
       " 419,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 881,\n",
       " 112,\n",
       " 8269,\n",
       " 2208,\n",
       " 420,\n",
       " 8807,\n",
       " 388,\n",
       " 1603,\n",
       " 385,\n",
       " 2352,\n",
       " 363,\n",
       " 3886,\n",
       " 114,\n",
       " 733,\n",
       " 1240,\n",
       " 388,\n",
       " 7423,\n",
       " 961,\n",
       " 112,\n",
       " 467,\n",
       " 1127,\n",
       " 439,\n",
       " 508,\n",
       " 2663,\n",
       " 385,\n",
       " 2597,\n",
       " 428,\n",
       " 5274,\n",
       " 11440,\n",
       " 654,\n",
       " 388,\n",
       " 644,\n",
       " 440,\n",
       " 419,\n",
       " 2383,\n",
       " 427,\n",
       " 764,\n",
       " 508,\n",
       " 2663,\n",
       " 385,\n",
       " 861,\n",
       " 712,\n",
       " 441,\n",
       " 419,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 480,\n",
       " 529,\n",
       " 1067,\n",
       " 114,\n",
       " 655,\n",
       " 578,\n",
       " 427,\n",
       " 585,\n",
       " 922,\n",
       " 2383,\n",
       " 112,\n",
       " 764,\n",
       " 2293,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 114,\n",
       " 100,\n",
       " 8162,\n",
       " 1908,\n",
       " 427,\n",
       " 363,\n",
       " 2087,\n",
       " 474,\n",
       " 7143,\n",
       " 517,\n",
       " 358,\n",
       " 14586,\n",
       " 605,\n",
       " 881,\n",
       " 585,\n",
       " 1908,\n",
       " 427,\n",
       " 713,\n",
       " 474,\n",
       " 1305,\n",
       " 420,\n",
       " 8807,\n",
       " 114,\n",
       " 1097,\n",
       " 718,\n",
       " 1011,\n",
       " 427,\n",
       " 1305,\n",
       " 420,\n",
       " 8807,\n",
       " 958,\n",
       " 2253,\n",
       " 112,\n",
       " 415,\n",
       " 993,\n",
       " 427,\n",
       " 713,\n",
       " 419,\n",
       " 746,\n",
       " 1305,\n",
       " 420,\n",
       " 8807,\n",
       " 114,\n",
       " 100,\n",
       " 1127,\n",
       " 1240,\n",
       " 388,\n",
       " 7423,\n",
       " 868,\n",
       " 112,\n",
       " 420,\n",
       " 3136,\n",
       " 743,\n",
       " 112,\n",
       " 7896,\n",
       " 112,\n",
       " 8807,\n",
       " 8161,\n",
       " 13185,\n",
       " 374,\n",
       " 13213,\n",
       " 726,\n",
       " 428,\n",
       " 5274,\n",
       " 11440,\n",
       " 430,\n",
       " 363,\n",
       " 818,\n",
       " 741,\n",
       " 114,\n",
       " 4000,\n",
       " 438,\n",
       " 14515,\n",
       " 1819,\n",
       " 358,\n",
       " 4387,\n",
       " 387,\n",
       " 8807,\n",
       " 452,\n",
       " 566,\n",
       " 15940,\n",
       " 2777,\n",
       " 20533,\n",
       " 112,\n",
       " 427,\n",
       " 363,\n",
       " 2087,\n",
       " 474,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 114,\n",
       " 11176,\n",
       " 609,\n",
       " 1908,\n",
       " 441,\n",
       " 474,\n",
       " 1026,\n",
       " 517,\n",
       " 358,\n",
       " 14586,\n",
       " 605,\n",
       " 873,\n",
       " 1097,\n",
       " 441,\n",
       " 2593,\n",
       " 571,\n",
       " 112,\n",
       " 474,\n",
       " 508,\n",
       " 11479,\n",
       " 114,\n",
       " 415,\n",
       " 993,\n",
       " 585,\n",
       " 648,\n",
       " 508,\n",
       " 11479,\n",
       " 881,\n",
       " 585,\n",
       " 524,\n",
       " 1908,\n",
       " 1302,\n",
       " 15509,\n",
       " 427,\n",
       " 441,\n",
       " 474,\n",
       " 1208,\n",
       " 7143,\n",
       " 517,\n",
       " 358,\n",
       " 14586,\n",
       " 605,\n",
       " 114,\n",
       " 100,\n",
       " 11016,\n",
       " 762,\n",
       " 648,\n",
       " 508,\n",
       " 3433,\n",
       " 1532,\n",
       " 647,\n",
       " 804,\n",
       " 363,\n",
       " 2057,\n",
       " 788,\n",
       " 474,\n",
       " 358,\n",
       " 3389,\n",
       " 2057,\n",
       " 788,\n",
       " 112,\n",
       " 388,\n",
       " 578,\n",
       " 112,\n",
       " 457,\n",
       " 750,\n",
       " 427,\n",
       " 358,\n",
       " 14586,\n",
       " 605,\n",
       " 851,\n",
       " 508,\n",
       " 1397,\n",
       " 363,\n",
       " 2087,\n",
       " 114,\n",
       " 415,\n",
       " 662,\n",
       " 689,\n",
       " 385,\n",
       " 861,\n",
       " 804,\n",
       " 363,\n",
       " 2057,\n",
       " 788,\n",
       " 474,\n",
       " 7143,\n",
       " 114,\n",
       " 457,\n",
       " 861,\n",
       " 884,\n",
       " 427,\n",
       " 1305,\n",
       " 420,\n",
       " 8807,\n",
       " 1696,\n",
       " 571,\n",
       " 2253,\n",
       " 114,\n",
       " 321,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6feded5f862e2f5b544fb63d3bb6c48e49c7ae1c966055a5942297c4c48158e1"
  },
  "kernelspec": {
   "display_name": "jax_notebook",
   "language": "python",
   "name": "jax_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
