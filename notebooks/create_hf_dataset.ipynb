{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datasets import load_dataset\n",
    "\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from copy import copy\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    FlaxAutoModelForSequenceClassification,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    TrainingArguments,\n",
    "    is_tensorboard_available,\n",
    ")\n",
    "\n",
    "from flax.training.common_utils import get_metrics, onehot, shard\n",
    "\n",
    "\n",
    "data_root = \"/kaggle/input/feedback-prize-effectiveness/\"\n",
    "train = pd.read_csv(\"/kaggle/input/feedback-prize-effectiveness/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}\n",
    "\n",
    "def _prepare_training_data_helper(args, tokenizer, df, is_train):\n",
    "    training_samples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        idx = row[\"essay_id\"]\n",
    "        discourse_text = row[\"discourse_text\"]\n",
    "        discourse_type = row[\"discourse_type\"]\n",
    "\n",
    "        if is_train:\n",
    "            filename = os.path.join(args.input, \"train\", idx + \".txt\")\n",
    "        else:\n",
    "            filename = os.path.join(args.input, \"test\", idx + \".txt\")\n",
    "\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            discourse_type + \" \" + discourse_text,\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512 ##TODO: update max_length\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "\n",
    "        sample = {\n",
    "            # \"discourse_id\": row[\"discourse_id\"],\n",
    "            \"input_ids\": input_ids,\n",
    "            # \"discourse_text\": discourse_text,\n",
    "            # \"essay_text\": text,\n",
    "            \"attention_mask\": encoded_text[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "        if \"token_type_ids\" in encoded_text:\n",
    "            sample[\"token_type_ids\"] = encoded_text[\"token_type_ids\"]\n",
    "\n",
    "        label = row[\"discourse_effectiveness\"]\n",
    "\n",
    "        sample[\"labels\"] = LABEL_MAPPING[label]\n",
    "\n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "def prepare_training_data(df, tokenizer, args, num_jobs, is_train):\n",
    "    training_samples = []\n",
    "\n",
    "    df_splits = np.array_split(df, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_training_data_helper)(args, tokenizer, df, is_train) for df in df_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        training_samples.extend(result)\n",
    "\n",
    "    return training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing FlaxBertForSequenceClassification: {('cls', 'predictions', 'bias'), ('cls', 'predictions', 'transform', 'dense', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'bias'), ('cls', 'predictions', 'transform', 'dense', 'kernel'), ('cls', 'predictions', 'transform', 'LayerNorm', 'scale')}\n",
      "- This IS expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: {('classifier', 'kernel'), ('bert', 'pooler', 'dense', 'bias'), ('classifier', 'bias'), ('bert', 'pooler', 'dense', 'kernel')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../configs\")\n",
    "cfg = copy(importlib.import_module(\"default_config\").cfg)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config = AutoConfig.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    num_labels=cfg.num_labels,\n",
    "    #finetuning_task=data_args.task_name,\n",
    "    #use_auth_token=True if cfg.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    use_fast=not cfg.use_slow_tokenizer,\n",
    "    #use_auth_token=True if cfg.use_auth_token else None,\n",
    ")\n",
    "model = FlaxAutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    config=config,\n",
    "    #use_auth_token=True if cfg.use_auth_token else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 101.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 107.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 86.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 88.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 107.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 108.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 108.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 110.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 101.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 102.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 552.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 93.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 94.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 97.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 102.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 102.60it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "https://symbolize.stripped_domain/r/?trace=7f774b3ed3f4,7f774b4410bf,7f&map= \n",
      "*** SIGTERM received by PID 535694 (TID 535694) on cpu 15 from PID 517862; stack trace: ***\n",
      "PC: @     0x7f774b3ed3f4  (unknown)  do_futex_wait.constprop.0\n",
      "    @     0x7f75c8f26c73        992  (unknown)\n",
      "    @     0x7f774b4410c0  (unknown)  (unknown)\n",
      "    @               0x80  (unknown)  (unknown)\n",
      "https://symbolize.stripped_domain/r/?trace=7f774b3ed3f4,7f75c8f26c72,7f774b4410bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f75b47f8000-7f75c92a7b70 \n",
      "E0608 14:35:59.407855  535694 coredump_hook.cc:320] RAW: Remote crash gathering disabled for SIGTERM.\n",
      "https://symbolize.stripped_domain/r/?trace=7f75c8f83cc3,7f774b4410bf,7f75c8e32666,7f75c8f3c92a,7f75c8f424cc,7f75c8f41072,7f75c8f40b29,7f75c92a0d0d,7f75c8f2770a,7f774b4410bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f75b47f8000-7f75c92a7b70 \n",
      "E0608 14:35:59.418091  535694 process_state.cc:1067] RAW: Signal 11 raised at PC: 0x7f75c8f83cc3 while already in FailureSignalHandler!\n",
      "E0608 14:35:59.418132  535694 process_state.cc:1102] RAW: Raising 11 signal with default behavior\n"
     ]
    }
   ],
   "source": [
    "val_data = prepare_training_data(train.iloc[range(0, 100, 5)], tokenizer, cfg, num_jobs=96, is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(val_data[0]['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 217/307 [00:00<00:00, 381.44it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 467.42it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 434.70it/s]\n",
      " 34%|███▍      | 104/307 [00:00<00:00, 484.59it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 490.22it/s]\n",
      " 74%|███████▍  | 228/307 [00:00<00:00, 457.05it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 405.02it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 438.87it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 454.25it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 418.80it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 441.63it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 409.15it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 463.65it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 415.38it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 416.33it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 415.51it/s]\n",
      " 59%|█████▉    | 181/307 [00:00<00:00, 572.56it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 433.26it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 405.52it/s]\n",
      " 46%|████▌     | 141/307 [00:00<00:00, 339.63it/s]\n",
      "  0%|          | 0/307 [00:00<?, ?it/s]444.65it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 408.73it/s]\n",
      "\n",
      "100%|██████████| 307/307 [00:00<00:00, 421.39it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 405.30it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 451.64it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 484.25it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 354.07it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 373.81it/s]\n",
      " 14%|█▎        | 42/306 [00:00<00:00, 418.89it/s]]\n",
      "100%|██████████| 307/307 [00:00<00:00, 500.43it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 447.66it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 460.64it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 512.35it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 559.49it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 414.68it/s]\n",
      "\n",
      "100%|██████████| 306/306 [00:00<00:00, 420.66it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 519.03it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 418.66it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 493.13it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 547.80it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 521.24it/s]\n",
      "\n",
      "100%|██████████| 306/306 [00:00<00:00, 526.74it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 493.98it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 501.38it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 456.57it/s]\n",
      " 19%|█▉        | 58/306 [00:00<00:00, 573.21it/s]]\n",
      "100%|██████████| 306/306 [00:00<00:00, 471.44it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 495.88it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 425.07it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 544.49it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 454.84it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 490.54it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 441.22it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 505.14it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 546.04it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 451.35it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 529.43it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 537.07it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 506.91it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 535.41it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 547.82it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 457.75it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 533.97it/s]\n",
      " 42%|████▏     | 128/306 [00:00<00:00, 560.10it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 548.52it/s]\n",
      " 72%|███████▏  | 220/306 [00:00<00:00, 451.59it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 540.78it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 451.33it/s]\n",
      " 63%|██████▎   | 192/306 [00:00<00:00, 489.10it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 475.44it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 519.16it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 552.62it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 456.02it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 490.73it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 523.11it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 514.33it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 497.92it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 485.77it/s]\n",
      " 86%|████████▌ | 263/306 [00:00<00:00, 457.30it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 479.03it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 508.17it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 436.84it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 506.39it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 382.96it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 354.08it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 366.11it/s]\n",
      " 66%|██████▌   | 202/306 [00:00<00:00, 332.63it/s]\n",
      " 95%|█████████▌| 291/306 [00:00<00:00, 376.75it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 378.99it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 347.48it/s]\n",
      " 90%|█████████ | 276/306 [00:00<00:00, 383.80it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 387.85it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 363.13it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 459.68it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 595.80it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 446.14it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 553.08it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 492.98it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 504.99it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 492.38it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 496.72it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 493.13it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 481.60it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 611.76it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 495.38it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 490.99it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 511.33it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 421.96it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 495.80it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 432.00it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 512.27it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 516.53it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 517.44it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 519.78it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 483.56it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 447.39it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 473.18it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 506.86it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 432.09it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 485.97it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 543.75it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 550.17it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 499.51it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 511.60it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 532.46it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 531.16it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 571.03it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 406.36it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 516.97it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 573.79it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 546.94it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 485.29it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 616.08it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 468.47it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 478.83it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 545.13it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 604.47it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 565.19it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 572.79it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 533.62it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 503.59it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 477.14it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 542.86it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 468.12it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 601.33it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 593.78it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 500.01it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 493.11it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 470.98it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 578.77it/s]\n",
      " 86%|████████▌ | 65/76 [00:00<00:00, 649.35it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 647.37it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 586.88it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 551.16it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 489.53it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 447.11it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 587.85it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 470.15it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 471.48it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 515.62it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 536.80it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 527.66it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 560.66it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 490.35it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 524.55it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 549.00it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 582.55it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 542.95it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 489.26it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 552.89it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 565.25it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 520.77it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 522.19it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 507.15it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 493.74it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 498.94it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 502.58it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 529.78it/s]\n",
      " 49%|████▊     | 37/76 [00:00<00:00, 366.10it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 388.96it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 404.41it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 408.59it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 402.60it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 377.53it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 401.76it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 459.06it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 335.07it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 354.64it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 430.51it/s]\n",
      "https://symbolize.stripped_domain/r/?trace=7f774b3ed3f4,7f774b4410bf,7f&map= \n",
      "*** SIGTERM received by PID 570785 (TID 570785) on cpu 70 from PID 517862; stack trace: ***\n",
      "PC: @     0x7f774b3ed3f4  (unknown)  do_futex_wait.constprop.0\n",
      "    @     0x7f75c8f26c73        992  (unknown)\n",
      "    @     0x7f774b4410c0  (unknown)  (unknown)\n",
      "    @               0x80  (unknown)  (unknown)\n",
      "https://symbolize.stripped_domain/r/?trace=7f774b3ed3f4,7f75c8f26c72,7f774b4410bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f75b47f8000-7f75c92a7b70 \n",
      "E0608 14:37:58.617154  570785 coredump_hook.cc:320] RAW: Remote crash gathering disabled for SIGTERM.\n",
      "https://symbolize.stripped_domain/r/?trace=7f75c8f83cc3,7f774b4410bf,7f75c8e32666,7f75c8f3c92a,7f75c8f424cc,7f75c8f41072,7f75c8f40b29,7f75c92a0d0d,7f75c8f2770a,7f774b4410bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f75b47f8000-7f75c92a7b70 \n",
      "E0608 14:37:58.619237  570785 process_state.cc:1067] RAW: Signal 11 raised at PC: 0x7f75c8f83cc3 while already in FailureSignalHandler!\n",
      "E0608 14:37:58.619247  570785 process_state.cc:1102] RAW: Raising 11 signal with default behavior\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Train: [    0     1     4 ... 36762 36763 36764]\n",
      "Valid: [    2     3     7 ... 36733 36736 36746]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "## stratified Kfold for train dataframe using discourse_type and discourse_effectiveness\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train, train[\"discourse_type\"], train[\"discourse_effectiveness\"])):\n",
    "\n",
    "    train_temp = train.iloc[train_index]\n",
    "    valid_temp = train.iloc[valid_index]\n",
    "\n",
    "    train_data = prepare_training_data(train_temp, tokenizer, cfg, num_jobs=96, is_train=True)\n",
    "    val_data = prepare_training_data(valid_temp, tokenizer, cfg, num_jobs=96, is_train=True)\n",
    "\n",
    "    df = pd.DataFrame.from_records(train_data)\n",
    "    df.to_json(f\"/kaggle/working/folds/train_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    df = pd.DataFrame.from_records(val_data)\n",
    "    df.to_json(f\"/kaggle/working/folds/valid_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    print(\"Fold:\", fold)\n",
    "    print(\"Train:\", train_index)\n",
    "    print(\"Valid:\", valid_index)\n",
    "    print(\"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [00:01<00:00, 527.15it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 535.80it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 541.70it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 554.34it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 517.09it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 504.91it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 501.29it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 505.08it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 495.78it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 527.82it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 576.84it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 552.58it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 623.94it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 539.16it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 582.45it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 558.21it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 638.66it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 562.49it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 582.94it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 584.09it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 605.94it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 593.88it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 528.22it/s]\n",
      " 76%|███████▌  | 695/919 [00:01<00:00, 596.28it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (9251 > 4096). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 919/919 [00:01<00:00, 563.25it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 606.08it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 572.22it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 591.34it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 521.93it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 530.30it/s]\n",
      "100%|██████████| 919/919 [00:02<00:00, 434.69it/s]\n",
      "100%|██████████| 919/919 [00:02<00:00, 435.70it/s]\n",
      "100%|██████████| 919/919 [00:02<00:00, 404.32it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 466.23it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 523.13it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 517.56it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 524.59it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 516.35it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 523.07it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 485.89it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 436.21it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 450.06it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 506.53it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 473.11it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 532.34it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 538.58it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 634.58it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 543.01it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 549.26it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 527.59it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 601.19it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 517.06it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 583.06it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 542.03it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 506.22it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 525.10it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 518.97it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 569.77it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 499.31it/s]\n",
      "  0%|          | 0/229 [00:00<?, ?it/s]67.64it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (9275 > 4096). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 229/229 [00:00<00:00, 521.73it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 464.20it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 491.96it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 407.18it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 362.84it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 415.79it/s]\n",
      "https://symbolize.stripped_domain/r/?trace=7f26ffdd18fd,7f26ffd7f0bf&map= \n",
      "*** SIGTERM received by PID 201426 (TID 201426) on cpu 51 from PID 88996; stack trace: ***\n",
      "PC: @     0x7f26ffdd18fd  (unknown)  (unknown)\n",
      "    @     0x7f261dfb7353        992  (unknown)\n",
      "    @     0x7f26ffd7f0c0  (unknown)  (unknown)\n",
      "    @ ... and at least 1 more frames\n",
      "https://symbolize.stripped_domain/r/?trace=7f26ffdd18fd,7f261dfb7352,7f26ffd7f0bf&map=a7dce6a9f5f70e08fccb1cadb4ea57b5:7f2609b31000-7f261e3312f0 \n",
      "E0607 18:44:49.296827  201426 coredump_hook.cc:320] RAW: Remote crash gathering disabled for SIGTERM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Train: [    0     1     4 ... 36762 36763 36764]\n",
      "Valid: [    2     3     7 ... 36733 36736 36746]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [00:01<00:00, 517.61it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 534.69it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 548.40it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 558.75it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 514.06it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 537.65it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 497.47it/s]\n",
      " 10%|█         | 94/919 [00:00<00:01, 483.61it/s]]"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# ## stratified Kfold for train dataframe using discourse_type and discourse_effectiveness\n",
    "# for fold, (train_index, valid_index) in enumerate(kf.split(train, train[\"discourse_type\"], train[\"discourse_effectiveness\"])):\n",
    "\n",
    "#     train_temp = train.iloc[train_index]\n",
    "#     valid_temp = train.iloc[valid_index]\n",
    "\n",
    "#     train_data = prepare_training_data(train_temp, tokenizer, cfg, num_jobs=32, is_train=True)\n",
    "#     val_data = prepare_training_data(valid_temp, tokenizer, cfg, num_jobs=32, is_train=True)\n",
    "\n",
    "#     df = pd.DataFrame.from_records(train_data)\n",
    "#     df.to_json(f\"/kaggle/working/folds/train_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "#     df = pd.DataFrame.from_records(val_data)\n",
    "#     df.to_json(f\"/kaggle/working/folds/valid_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "#     print(\"Fold:\", fold)\n",
    "#     print(\"Train:\", train_index)\n",
    "#     print(\"Valid:\", valid_index)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data collator with dynamic padding\n",
    "# def train_data_collator(rng:)\n",
    "import jax\n",
    "import datasets\n",
    "from typing import Any, Callable, Dict, Optional, Tuple\n",
    "\n",
    "rng = jax.random.PRNGKey(1)#cfg.seed)\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "Array = Any\n",
    "Dataset = datasets.arrow_dataset.Dataset\n",
    "PRNGKey = Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_collator(rng: PRNGKey, dataset: Dataset, batch_size: int):\n",
    "    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    perms = jax.random.permutation(rng, len(dataset))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    for perm in perms:\n",
    "        batch = dataset[perm]\n",
    "        discourse_id, input_ids, labels = dataset[perm]['discourse_id'], dataset[perm]['input_ids'], dataset[perm]['label']\n",
    "        batch.pop(\"discourse_id\", None)\n",
    "        batch = {\"input_ids\": np.array(input_ids), \"mask\": [np.ones_like(x) for x in input_ids], \"label\": np.array(labels)}\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in batch[\"input_ids\"]])\n",
    "        # add padding\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            batch[\"input_ids\"] = [s + (batch_max - len(s)) * [tokenizer.pad_token_id] for s in batch[\"input_ids\"]]\n",
    "        else:\n",
    "            batch[\"input_ids\"] = [(batch_max - len(s)) * [tokenizer.pad_token_id] + s for s in batch[\"input_ids\"]]\n",
    "\n",
    "        batch['input_ids'] = np.stack(batch['input_ids'])\n",
    "        \n",
    "        masks = np.zeros_like(batch['input_ids'])\n",
    "        masks[batch['input_ids'] != tokenizer.pad_token_id] = 1\n",
    "        batch['mask'] = masks\n",
    "\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "        batch = shard(batch)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-e12ff2ea41160c33\n",
      "WARNING:datasets.builder:Reusing dataset json (/root/.cache/huggingface/datasets/json/default-e12ff2ea41160c33/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"json\", data_files=\"/kaggle/working/folds/valid_0.jsonl\", split=\"train\")\n",
    "train_loader = train_data_collator(rng, train_dataset, cfg.per_device_train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "(4, 938)\n",
      "{'input_ids': array([[  898,  4461, 39341, ...,     0,     0,     0],\n",
      "       [21360,  1507,   712, ...,     0,     0,     0],\n",
      "       [20217,  4055,  8090, ...,   100,   321,   100],\n",
      "       [23259,   415,   993, ...,     0,     0,     0]]), 'mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0]]), 'label': array([0, 1, 1, 0])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207481/2323644318.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  batch = {\"input_ids\": np.array(input_ids), \"mask\": [np.ones_like(x) for x in input_ids], \"label\": np.array(labels)}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([21360,  1507,   712,   446,   993,   647,   441,   112,   441,\n",
       "         1345,   756,   408,  1466,   385,  1350,   707,   889,   385,\n",
       "        22490,   707,   114,   415,  1755,   662,  3012,   427,   446,\n",
       "          582, 36538,   529,   726,   881,  1413,   817,  2596,  1755,\n",
       "          446,   937,   571,   866,   385,   524,   385,  1831,   452,\n",
       "          878,  2545,   858,  1407,   715,   114,   988,   764,   578,\n",
       "          611,   385,   446,   114,   321,  1776, 32742,   112,   100,\n",
       "          141,   993,   427,  2551,   631,   419,   358,  1466,  2551,\n",
       "          112,   881,   712,   456,   991,   456,   457,   937,   571,\n",
       "          524,   775,  2786,  9613,   604,   388,  1499,   889,   457,\n",
       "          916,   408,  1599,   385,   524,   707,   604,  1242,  1580,\n",
       "          741,   391, 10066,   741,   114,   988,   452,  2786,  9613,\n",
       "          508,   953, 32328,   385,   408,   420,   388,  1625,   494,\n",
       "         1074,  1939,   363,  2545,   866,   385,   524,   707,   604,\n",
       "          388,  1499,   114,  1507,   712,   446,   648,   385,  1350,\n",
       "          707,   792,   388,  1580,   741,   494, 10066,   112,   889,\n",
       "         1413,   993,   427,  2545,  3737,   571,   524,   385,  2323,\n",
       "          707,   604,   388,  1499,   114,  1220,   815,   756,   524,\n",
       "          707,   388,  1580,   741,   114,   988,   363,   618,   363,\n",
       "         2545, 18649,   524,   707,   604,   480,   578,   889,   363,\n",
       "          618,   713,   756,  1117,   385,   880,   707,   388,  1499,\n",
       "          112,   719,   585,   490,  4486,   385,   408,  4774,   585,\n",
       "          582,   408, 36735,   420,   612,  9613,   494,   420,   363,\n",
       "         5331,  2528,   114,  1421,   815,   888,   707,  2139,  4333,\n",
       "         9660,   612,   388,   391,   524,   385, 41855,   441,   363,\n",
       "         1407,   715,   114,   100,  1633,   446,   662,  1350,  2786,\n",
       "         9613,   112,  3964,   363,  2545,   582,  9245,   612,  4774,\n",
       "          618,   391,  1783,   866,   385,   567, 25916,   391,  5355,\n",
       "         2528,   387,  1820,   612,  2786,  9613,   604,   114,  1507,\n",
       "          712,   446,   993,   647,   441,   112,   441,  1345,   756,\n",
       "          408,  1466,   385,  1350,   707,   889,   385, 22490,   707,\n",
       "          114,   415,  1755,   662,  3012,   427,   446,   582, 36538,\n",
       "          529,   726,   881,  1413,   817,  2596,  1755,   446,   937,\n",
       "          571,   866,   385,   524,   385,  1831,   452,   878,  2545,\n",
       "          858,  1407,   715,   114,   988,   764,   578,   611,   385,\n",
       "          446,   114,   100,   141,   567,  1208,   993,   427,   441,\n",
       "          662,   408,   936,  1466,   385,  1350,   707,   881,  3461,\n",
       "          388,  1499,   938,   717,  2561,   391,  1413,   524,   775,\n",
       "         2786,  9613,   604,  1413,  1713,   745,  3811,  3737,   571,\n",
       "          131,   988,   718,  2545,   792,   567,   441,   689,  1853,\n",
       "          494,  5504,   689,   603,   576,   685,   585,   582,   756,\n",
       "          524,   441,   604,   578,   363,   741,   388,   891,  1499,\n",
       "          391,   508,  1515,  3342,   114,  1507,   427,   756,  1939,\n",
       "          707,  2139,   391,   524,   385,   321, 48555,  2380,   578,\n",
       "          726,   114,   100,  3338,  7956,  2786,  9613,   792,   388,\n",
       "         1580,   741,   391, 10066,   439,   662,  5558,  1588,   718,\n",
       "          387,   363,  2545, 19152,   391, 14580,   114,  1220,   815,\n",
       "          880,   707,   719,   612,   508,   388,  1499,   391,   480,\n",
       "        10066,   114,  5556,   387,   388,  1499,   391,  8448,  2174,\n",
       "          114,   415,   817,  2596,  1755,   427,   850,   387,   363,\n",
       "         7900,   524,   612,  9613,   604,   624,   427,   756,  3708,\n",
       "          363, 12977,   385,  2545,   385,  1112,   604,   612,   439,\n",
       "          114,   100,  2162,  1413,   817,  1208,  2212,   385,  1011,\n",
       "          419,   427,   446,   916,  1350,   707,   508, 22490,   707,\n",
       "          114,  1507,   712,   446,   662,  1112,   358,  1319,   391,\n",
       "        26641,   529,   889,   441,   662,   888,   578,   363,  2545,\n",
       "         5657,   603,   468, 24906,   157,  5246,   100,  2497,  3488,\n",
       "          756, 26641,   114,  7053,   446,   865,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['mask'][1], batch['input_ids'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = jax.profiler.start_server(9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jaxlib.xla_extension.profiler.ProfilerServer at 0x7f7968dbc370>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6feded5f862e2f5b544fb63d3bb6c48e49c7ae1c966055a5942297c4c48158e1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('jax': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
