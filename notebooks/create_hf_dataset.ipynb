{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/jax/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datasets import load_dataset\n",
    "\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from copy import copy\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    FlaxAutoModelForSequenceClassification,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    TrainingArguments,\n",
    "    is_tensorboard_available,\n",
    ")\n",
    "\n",
    "from flax.training.common_utils import get_metrics, onehot, shard\n",
    "\n",
    "\n",
    "data_root = \"/kaggle/input/feedback-prize-effectiveness/\"\n",
    "train = pd.read_csv(\"/kaggle/input/feedback-prize-effectiveness/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}\n",
    "\n",
    "def _prepare_training_data_helper(args, tokenizer, df, is_train):\n",
    "    training_samples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        idx = row[\"essay_id\"]\n",
    "        discourse_text = row[\"discourse_text\"]\n",
    "        discourse_type = row[\"discourse_type\"]\n",
    "\n",
    "        if is_train:\n",
    "            filename = os.path.join(args.input, \"train\", idx + \".txt\")\n",
    "        else:\n",
    "            filename = os.path.join(args.input, \"test\", idx + \".txt\")\n",
    "\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            discourse_type + \" \" + discourse_text,\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=1024 ##TODO: update max_length\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "\n",
    "        sample = {\n",
    "            \"discourse_id\": row[\"discourse_id\"],\n",
    "            \"input_ids\": input_ids,\n",
    "            # \"discourse_text\": discourse_text,\n",
    "            # \"essay_text\": text,\n",
    "            \"attention_mask\": encoded_text[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "        if \"token_type_ids\" in encoded_text:\n",
    "            sample[\"token_type_ids\"] = encoded_text[\"token_type_ids\"]\n",
    "\n",
    "        try:\n",
    "            label = row[\"discourse_effectiveness\"]\n",
    "            sample[\"labels\"] = LABEL_MAPPING[label]\n",
    "        except:\n",
    "            sample[\"labels\"] = 0\n",
    "        \n",
    "\n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "def prepare_training_data(df, tokenizer, args, num_jobs, is_train):\n",
    "    training_samples = []\n",
    "\n",
    "    df_splits = np.array_split(df, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_training_data_helper)(args, tokenizer, df, is_train) for df in df_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        training_samples.extend(result)\n",
    "\n",
    "    return training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing FlaxBigBirdForSequenceClassification: {('cls', 'seq_relationship', 'bias'), ('cls', 'predictions', 'transform', 'dense', 'bias'), ('cls', 'seq_relationship', 'kernel'), ('cls', 'predictions', 'bias'), ('cls', 'predictions', 'transform', 'dense', 'kernel'), ('cls', 'predictions', 'transform', 'LayerNorm', 'scale'), ('cls', 'predictions', 'transform', 'LayerNorm', 'bias')}\n",
      "- This IS expected if you are initializing FlaxBigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: {('classifier', 'dense', 'bias'), ('classifier', 'out_proj', 'bias'), ('classifier', 'out_proj', 'kernel'), ('classifier', 'dense', 'kernel')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../configs\")\n",
    "cfg = copy(importlib.import_module(\"default_config\").cfg)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config = AutoConfig.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    num_labels=cfg.num_labels,\n",
    "    #finetuning_task=data_args.task_name,\n",
    "    #use_auth_token=True if cfg.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    use_fast=not cfg.use_slow_tokenizer,\n",
    "    #use_auth_token=True if cfg.use_auth_token else None,\n",
    ")\n",
    "model = FlaxAutoModelForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    config=config,\n",
    "    #use_auth_token=True if cfg.use_auth_token else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 87.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 99.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 68.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 64.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 99.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 95.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 101.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 75.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 83.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 94.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 87.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 102.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 84.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 88.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 101.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 96.55it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "https://symbolize.stripped_domain/r/?trace=7f16b57153f4,7f16b57690bf,7f&map= \n",
      "*** SIGTERM received by PID 760967 (TID 760967) on cpu 41 from PID 759496; stack trace: ***\n",
      "PC: @     0x7f16b57153f4  (unknown)  do_futex_wait.constprop.0\n",
      "    @     0x7f1533257c73        992  (unknown)\n",
      "    @     0x7f16b57690c0  (unknown)  (unknown)\n",
      "    @               0x80  (unknown)  (unknown)\n",
      "https://symbolize.stripped_domain/r/?trace=7f16b57153f4,7f1533257c72,7f16b57690bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f151eb29000-7f15335d8b70 \n",
      "E0608 20:25:37.410636  760967 coredump_hook.cc:320] RAW: Remote crash gathering disabled for SIGTERM.\n",
      "https://symbolize.stripped_domain/r/?trace=7f15332b4cc3,7f16b57690bf,7f1533163666,7f153326d92a,7f15332734cc,7f1533272072,7f1533271b29,7f15335d1d0d,7f153325870a,7f16b57690bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f151eb29000-7f15335d8b70 \n",
      "E0608 20:25:37.412906  760967 process_state.cc:1067] RAW: Signal 11 raised at PC: 0x7f15332b4cc3 while already in FailureSignalHandler!\n",
      "E0608 20:25:37.412936  760967 process_state.cc:1102] RAW: Raising 11 signal with default behavior\n"
     ]
    }
   ],
   "source": [
    "val_data = prepare_training_data(train.iloc[range(0, 100, 5)], tokenizer, cfg, num_jobs=96, is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/kaggle/working/notebooks/create_hf_dataset.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/create_hf_dataset.ipynb#ch0000038vscode-remote?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39marray(val_data[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_data' is not defined"
     ]
    }
   ],
   "source": [
    "np.array(val_data[0]['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:00<00:00, 579.38it/s]\n",
      "\n",
      "100%|██████████| 307/307 [00:00<00:00, 466.08it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 575.03it/s]\n",
      " 36%|███▌      | 109/307 [00:00<00:00, 545.23it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 514.95it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 558.82it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 481.35it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 521.07it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 522.42it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 526.79it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 557.86it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 524.05it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 479.98it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 519.72it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 467.63it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 493.20it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 526.79it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 468.95it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 539.26it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 494.06it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 533.63it/s]\n",
      " 25%|██▍       | 76/307 [00:00<00:00, 380.66it/s]]\n",
      "100%|██████████| 307/307 [00:00<00:00, 497.08it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 573.14it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 511.52it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 405.89it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 526.38it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 591.25it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 517.73it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 497.16it/s]\n",
      " 79%|███████▉  | 244/307 [00:00<00:00, 609.83it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 557.51it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 640.33it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 506.78it/s]\n",
      "100%|██████████| 307/307 [00:00<00:00, 523.61it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 506.74it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 626.18it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 499.89it/s]\n",
      " 90%|█████████ | 276/306 [00:00<00:00, 653.61it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 657.31it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 608.97it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 521.76it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 615.63it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 633.93it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 531.88it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 590.30it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 558.70it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 517.57it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 574.35it/s]\n",
      "  0%|          | 0/306 [00:00<?, ?it/s]01.55it/s]]\n",
      "100%|██████████| 306/306 [00:00<00:00, 627.71it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 629.57it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 527.45it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 531.56it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 580.31it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 536.15it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 535.94it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 610.36it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 543.37it/s]\n",
      "  0%|          | 0/306 [00:00<?, ?it/s]558.61it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 570.28it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 581.00it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 588.31it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 522.91it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 518.08it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 478.31it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 586.36it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 588.24it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 594.55it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 512.08it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 552.45it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 531.73it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 562.23it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 615.58it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 530.64it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 478.86it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 524.50it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 584.32it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 559.48it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 550.10it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 562.83it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 515.80it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 411.70it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 567.24it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 581.58it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 383.26it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 376.03it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 420.43it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 374.58it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 361.70it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 394.33it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 462.58it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 348.11it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 364.98it/s]\n",
      "100%|██████████| 306/306 [00:00<00:00, 410.21it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 464.85it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 545.14it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 408.49it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 523.67it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 453.97it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 461.06it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 493.67it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 481.16it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 454.24it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 460.45it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 483.61it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 461.47it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 437.14it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 408.55it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 432.12it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 460.78it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 397.24it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 475.28it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 464.78it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 485.72it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 463.04it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 461.62it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 426.45it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 456.11it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 410.25it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 406.38it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 464.69it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 505.20it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 512.96it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 468.43it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 468.75it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 429.03it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 501.50it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 518.03it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 429.60it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 452.31it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 540.49it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 472.42it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 460.25it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 584.08it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 539.20it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 530.50it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 519.70it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 541.24it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 465.75it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 529.48it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 501.22it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 473.13it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 448.24it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 585.67it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 433.34it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 602.75it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 522.80it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 452.20it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 470.66it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 434.48it/s]\n",
      "100%|██████████| 77/77 [00:00<00:00, 530.16it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 463.14it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 610.39it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 526.02it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 545.78it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 545.79it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 557.16it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 528.62it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 443.54it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 438.29it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 482.25it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 509.49it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 514.42it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 540.18it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 477.46it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 485.03it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 519.63it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 554.22it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 518.37it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 427.47it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 493.13it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 515.93it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 490.79it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 512.03it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 483.02it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 468.82it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 428.82it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 478.03it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 430.65it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 486.24it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 367.41it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 375.15it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 329.58it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 374.50it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 352.63it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 370.65it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 315.98it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 417.31it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 331.68it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 400.96it/s]\n",
      "https://symbolize.stripped_domain/r/?trace=7f42b307fa65,7f42b30d30bf&map=https://symbolize.stripped_domain/r/?trace=7f42b307f3f4,7f42b30d30bf,7f&map= \n",
      " \n",
      "*** SIGTERM received by PID 859787 (TID 859787) on cpu 0 from PID 847303; stack trace: ***\n",
      "*** SIGTERM received by PID 859783 (TID 859783) on cpu 35 from PID 847303; stack trace: ***\n",
      "PC: @     0x7f42b307fa65  (unknown)  sem_post@@GLIBC_2.2.5\n",
      "    @     0x7f4130bb8c73        992  (unknown)\n",
      "PC: @     0x7f42b307f3f4  (unknown)  do_futex_wait.constprop.0\n",
      "    @     0x7f4130bb8c73        992  (unknown)\n",
      "    @     0x7f42b30d30c0  (unknown)  (unknown)\n",
      "https://symbolize.stripped_domain/r/?trace=7f42b307fa65,7f4130bb8c72,7f42b30d30bf&map=abc33f1bfca16f4e7d925d4248b4beb3:7f411c48a000-7f4130f39b70 \n",
      "    @     0x7f42b30d30c0  (unknown)  (unknown)\n",
      "E0609 08:45:46.469137  859787 coredump_hook.cc:320] RAW: Remote crash gathering disabled for SIGTERM.\n",
      "https://symbolize.stripped_domain/r/?trace=7f4130c15cc3,7f42b30d30bf,7f4130ac4666,7f4130bce92a,7f4130bd44cc,7f4130bd3072,7f4130bd2b29,7f4130f32d0d,7f4130bb970a,7f42b30d30bf&map=abc33f1bfca16f4e7d925d4248b4beb3:7f411c48a000-7f4130f39b70 \n",
      "E0609 08:45:46.471559  859787 process_state.cc:1067] RAW: Signal 11 raised at PC: 0x7f4130c15cc3 while already in FailureSignalHandler!\n",
      "E0609 08:45:46.471574  859787 process_state.cc:1102] RAW: Raising 11 signal with default behavior\n",
      "    @               0x80  (unknown)  (unknown)\n",
      "https://symbolize.stripped_domain/r/?trace=7f42b307f3f4,7f4130bb8c72,7f42b30d30bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f411c48a000-7f4130f39b70 \n",
      "E0609 08:45:46.472856  859783 coredump_hook.cc:320] RAW: Remote crash gathering disabled for SIGTERM.\n",
      "https://symbolize.stripped_domain/r/?trace=7f4130c15cc3,7f42b30d30bf,7f4130ac4666,7f4130bce92a,7f4130bd44cc,7f4130bd3072,7f4130bd2b29,7f4130f32d0d,7f4130bb970a,7f42b30d30bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f411c48a000-7f4130f39b70 \n",
      "E0609 08:45:46.474917  859783 process_state.cc:1067] RAW: Signal 11 raised at PC: 0x7f4130c15cc3 while already in FailureSignalHandler!\n",
      "E0609 08:45:46.474927  859783 process_state.cc:1102] RAW: Raising 11 signal with default behavior\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Train: [    0     1     4 ... 36762 36763 36764]\n",
      "Valid: [    2     3     7 ... 36733 36736 36746]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "## stratified Kfold for train dataframe using discourse_type and discourse_effectiveness\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train, train[\"discourse_type\"], train[\"discourse_effectiveness\"])):\n",
    "\n",
    "    train_temp = train.iloc[train_index]\n",
    "    valid_temp = train.iloc[valid_index]\n",
    "\n",
    "    train_data = prepare_training_data(train_temp, tokenizer, cfg, num_jobs=96, is_train=True)\n",
    "    val_data = prepare_training_data(valid_temp, tokenizer, cfg, num_jobs=96, is_train=True)\n",
    "\n",
    "    df = pd.DataFrame.from_records(train_data)\n",
    "    df.to_json(f\"/kaggle/working/folds/train_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    df = pd.DataFrame.from_records(val_data)\n",
    "    df.to_json(f\"/kaggle/working/folds/valid_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    print(\"Fold:\", fold)\n",
    "    print(\"Train:\", train_index)\n",
    "    print(\"Valid:\", valid_index)\n",
    "    print(\"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 85.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 89.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 86.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 85.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 83.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 84.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 84.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 89.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 85.13it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "https://symbolize.stripped_domain/r/?trace=7f9f754663f4,7f9f754ba0bf,7f&map= \n",
      "*** SIGTERM received by PID 876059 (TID 876059) on cpu 72 from PID 874545; stack trace: ***\n",
      "PC: @     0x7f9f754663f4  (unknown)  do_futex_wait.constprop.0\n",
      "    @     0x7f9df2fa7c73        992  (unknown)\n",
      "    @     0x7f9f754ba0c0  (unknown)  (unknown)\n",
      "    @               0x80  (unknown)  (unknown)\n",
      "https://symbolize.stripped_domain/r/?trace=7f9f754663f4,7f9df2fa7c72,7f9f754ba0bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f9dde879000-7f9df3328b70 \n",
      "E0609 09:17:10.231875  876059 coredump_hook.cc:320] RAW: Remote crash gathering disabled for SIGTERM.\n",
      "https://symbolize.stripped_domain/r/?trace=7f9df3004cc3,7f9f754ba0bf,7f9df2eb3666,7f9df2fbd92a,7f9df2fc34cc,7f9df2fc2072,7f9df2fc1b29,7f9df3321d0d,7f9df2fa870a,7f9f754ba0bf,7f&map=abc33f1bfca16f4e7d925d4248b4beb3:7f9dde879000-7f9df3328b70 \n",
      "E0609 09:17:10.251645  876059 process_state.cc:1067] RAW: Signal 11 raised at PC: 0x7f9df3004cc3 while already in FailureSignalHandler!\n",
      "E0609 09:17:10.251685  876059 process_state.cc:1102] RAW: Raising 11 signal with default behavior\n"
     ]
    }
   ],
   "source": [
    "## generate test dataset\n",
    "test = pd.read_csv(\"/kaggle/input/feedback-prize-effectiveness/test.csv\")\n",
    "test_data = prepare_training_data(test, tokenizer, cfg, num_jobs=96, is_train=False)\n",
    "df = pd.DataFrame.from_records(test_data)\n",
    "df.to_json(f\"/kaggle/working/folds/test.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [00:01<00:00, 527.15it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 535.80it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 541.70it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 554.34it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 517.09it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 504.91it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 501.29it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 505.08it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 495.78it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 527.82it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 576.84it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 552.58it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 623.94it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 539.16it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 582.45it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 558.21it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 638.66it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 562.49it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 582.94it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 584.09it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 605.94it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 593.88it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 528.22it/s]\n",
      " 76%|███████▌  | 695/919 [00:01<00:00, 596.28it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (9251 > 4096). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 919/919 [00:01<00:00, 563.25it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 606.08it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 572.22it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 591.34it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 521.93it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 530.30it/s]\n",
      "100%|██████████| 919/919 [00:02<00:00, 434.69it/s]\n",
      "100%|██████████| 919/919 [00:02<00:00, 435.70it/s]\n",
      "100%|██████████| 919/919 [00:02<00:00, 404.32it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 466.23it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 523.13it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 517.56it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 524.59it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 516.35it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 523.07it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 485.89it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 436.21it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 450.06it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 506.53it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 473.11it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 532.34it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 538.58it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 634.58it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 543.01it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 549.26it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 527.59it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 601.19it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 517.06it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 583.06it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 542.03it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 506.22it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 525.10it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 518.97it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 569.77it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 499.31it/s]\n",
      "  0%|          | 0/229 [00:00<?, ?it/s]67.64it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (9275 > 4096). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 229/229 [00:00<00:00, 521.73it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 464.20it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 491.96it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 407.18it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 362.84it/s]\n",
      "100%|██████████| 229/229 [00:00<00:00, 415.79it/s]\n",
      "https://symbolize.stripped_domain/r/?trace=7f26ffdd18fd,7f26ffd7f0bf&map= \n",
      "*** SIGTERM received by PID 201426 (TID 201426) on cpu 51 from PID 88996; stack trace: ***\n",
      "PC: @     0x7f26ffdd18fd  (unknown)  (unknown)\n",
      "    @     0x7f261dfb7353        992  (unknown)\n",
      "    @     0x7f26ffd7f0c0  (unknown)  (unknown)\n",
      "    @ ... and at least 1 more frames\n",
      "https://symbolize.stripped_domain/r/?trace=7f26ffdd18fd,7f261dfb7352,7f26ffd7f0bf&map=a7dce6a9f5f70e08fccb1cadb4ea57b5:7f2609b31000-7f261e3312f0 \n",
      "E0607 18:44:49.296827  201426 coredump_hook.cc:320] RAW: Remote crash gathering disabled for SIGTERM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Train: [    0     1     4 ... 36762 36763 36764]\n",
      "Valid: [    2     3     7 ... 36733 36736 36746]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [00:01<00:00, 517.61it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 534.69it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 548.40it/s]\n",
      "100%|██████████| 920/920 [00:01<00:00, 558.75it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 514.06it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 537.65it/s]\n",
      "100%|██████████| 919/919 [00:01<00:00, 497.47it/s]\n",
      " 10%|█         | 94/919 [00:00<00:01, 483.61it/s]]"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# ## stratified Kfold for train dataframe using discourse_type and discourse_effectiveness\n",
    "# for fold, (train_index, valid_index) in enumerate(kf.split(train, train[\"discourse_type\"], train[\"discourse_effectiveness\"])):\n",
    "\n",
    "#     train_temp = train.iloc[train_index]\n",
    "#     valid_temp = train.iloc[valid_index]\n",
    "\n",
    "#     train_data = prepare_training_data(train_temp, tokenizer, cfg, num_jobs=32, is_train=True)\n",
    "#     val_data = prepare_training_data(valid_temp, tokenizer, cfg, num_jobs=32, is_train=True)\n",
    "\n",
    "#     df = pd.DataFrame.from_records(train_data)\n",
    "#     df.to_json(f\"/kaggle/working/folds/train_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "#     df = pd.DataFrame.from_records(val_data)\n",
    "#     df.to_json(f\"/kaggle/working/folds/valid_{fold}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "#     print(\"Fold:\", fold)\n",
    "#     print(\"Train:\", train_index)\n",
    "#     print(\"Valid:\", valid_index)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data collator with dynamic padding\n",
    "# def train_data_collator(rng:)\n",
    "import jax\n",
    "import datasets\n",
    "from typing import Any, Callable, Dict, Optional, Tuple\n",
    "\n",
    "rng = jax.random.PRNGKey(1)#cfg.seed)\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "Array = Any\n",
    "Dataset = datasets.arrow_dataset.Dataset\n",
    "PRNGKey = Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_collator(rng: PRNGKey, dataset: Dataset, batch_size: int):\n",
    "    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    perms = jax.random.permutation(rng, len(dataset))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    for perm in perms:\n",
    "        batch = dataset[perm]\n",
    "        discourse_id, input_ids, labels = dataset[perm]['discourse_id'], dataset[perm]['input_ids'], dataset[perm]['label']\n",
    "        batch.pop(\"discourse_id\", None)\n",
    "        batch = {\"input_ids\": np.array(input_ids), \"mask\": [np.ones_like(x) for x in input_ids], \"label\": np.array(labels)}\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in batch[\"input_ids\"]])\n",
    "        # add padding\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            batch[\"input_ids\"] = [s + (batch_max - len(s)) * [tokenizer.pad_token_id] for s in batch[\"input_ids\"]]\n",
    "        else:\n",
    "            batch[\"input_ids\"] = [(batch_max - len(s)) * [tokenizer.pad_token_id] + s for s in batch[\"input_ids\"]]\n",
    "\n",
    "        batch['input_ids'] = np.stack(batch['input_ids'])\n",
    "        \n",
    "        masks = np.zeros_like(batch['input_ids'])\n",
    "        masks[batch['input_ids'] != tokenizer.pad_token_id] = 1\n",
    "        batch['mask'] = masks\n",
    "\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "        batch = shard(batch)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-58f0581432597fe4\n",
      "WARNING:datasets.builder:Reusing dataset json (/root/.cache/huggingface/datasets/json/default-58f0581432597fe4/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"json\", data_files=\"/kaggle/working/folds/valid_0.jsonl\", split=\"train\")\n",
    "train_loader = train_data_collator(rng, train_dataset, cfg.per_device_train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform. On my perspective, I think that the face is a natural landform because I dont think that there is any life on Mars. In these next few paragraphs, I'll be talking about how I think that is is a natural landform<unk>I think that the face is a natural landform because there is no life on Mars that we have descovered yet. If life was on Mars, we would know by now. The reason why I think it is a natural landform because, nobody live on Mars in order to create the figure. It says in paragraph 9, \"It's not easy to target Cydonia,\" in which he is saying that its not easy to know if it is a natural landform at this point. In all that they're saying, its probably a natural landform.<unk>People thought that the face was formed by alieans because they thought that there was life on Mars. though some say that life on Mars does exist, I think that there is no life on Mars.<unk>It says in paragraph 7, on April 5, 1998, Mars Global Surveyor flew over Cydonia for the first time. Michael Malin took a picture of Mars with his Orbiter Camera, that the face was a natural landform. Everyone who thought it was made by alieans even though it wasn't, was not satisfied. I think they were not satisfied because they have thought since 1976 that it was really formed by alieans.<unk>Though people were not satified about how the landform was a natural landform, in all, we new that alieans did not form the face. I would like to know how the landform was formed. we know now that life on Mars doesn't exist. <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "## decode text using tokenizer\n",
    "text = train_dataset[0]['input_ids']\n",
    "text = tokenizer.decode(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6feded5f862e2f5b544fb63d3bb6c48e49c7ae1c966055a5942297c4c48158e1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('jax': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
