{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datasets import load_dataset\n",
    "\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from copy import copy\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    FlaxAutoModel,\n",
    "    FlaxT5Model,\n",
    "    FlaxAutoModelForSequenceClassification,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    TrainingArguments,\n",
    "    is_tensorboard_available,\n",
    ")\n",
    "\n",
    "from flax.training.common_utils import get_metrics, onehot, shard\n",
    "\n",
    "\n",
    "data_root = \"/kaggle/input/feedback-prize-effectiveness/\"\n",
    "train = pd.read_csv(\"/kaggle/input/feedback-prize-effectiveness/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../configs\")\n",
    "cfg = copy(importlib.import_module(\"elu_config\").cfg)\n",
    "cfg.model_name_or_path = \"facebook/opt-125m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "sys.path.append(\"../models/\")\n",
    "# FeedbackJax = importlib.import_module(\"gpt2_model\").FeedbackJax\n",
    "# Net(cfg, config_path=None, pretrained=True)\n",
    "\n",
    "\n",
    "# FeedbackJax.from_text_pretrained(\n",
    "#     cfg.model_name_or_path,\n",
    "#     seed=cfg.seed,\n",
    "#     dtype=jnp.float32,\n",
    "#     text_from_pt=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "WARNING:datasets.builder:Using custom data configuration default-065504fbdee85a83\n",
      "WARNING:datasets.builder:Reusing dataset json (/root/.cache/huggingface/datasets/json/default-065504fbdee85a83/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    }
   ],
   "source": [
    "## data collator with dynamic padding\n",
    "# def train_data_collator(rng:)\n",
    "import jax\n",
    "import datasets\n",
    "from typing import Any, Callable, Dict, Optional, Tuple\n",
    "\n",
    "rng = jax.random.PRNGKey(1)#cfg.seed)\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "Array = Any\n",
    "Dataset = datasets.arrow_dataset.Dataset\n",
    "PRNGKey = Any\n",
    "\n",
    "\n",
    "def train_data_collator(rng: PRNGKey, dataset: Dataset, batch_size: int):\n",
    "    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    perms = jax.random.permutation(rng, len(dataset))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    for perm in perms:\n",
    "        batch = dataset[perm]\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "        #batch = shard(batch)\n",
    "\n",
    "        yield batch\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=\"/kaggle/working/folds/valid_0.jsonl\", split=\"train\")\n",
    "train_loader = train_data_collator(rng, train_dataset, cfg.per_device_train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 512)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['input_ids'].shape)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import flax\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from configuration_hybrid_clip import FeedbackJaxConfig\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "from transformers import FLAX_MODEL_MAPPING, FlaxCLIPVisionModel\n",
    "from transformers.modeling_flax_utils import FlaxPreTrainedModel\n",
    "from transformers.models.clip.modeling_flax_clip import FlaxCLIPOutput\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "from transformers.utils import ModelOutput\n",
    "from transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxBaseModelOutputWithPooling\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class FeedbackJaxOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits:(`jnp.ndarray` of shape `(will see)`):\n",
    "    \"\"\"\n",
    "\n",
    "    logits: jnp.ndarray = None\n",
    "    text_outputs: jnp.array = None\n",
    "    \n",
    "    def to_tuple(self) -> Tuple[Any]:\n",
    "        return tuple(\n",
    "            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n",
    "            for k in self.keys()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class FeedbackJaxModule(nn.Module):\n",
    "    config: FeedbackJaxConfig\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "    freeze_backbones: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        text_config = self.config.text_config\n",
    "\n",
    "        self.projection_dim = self.config.projection_dim\n",
    "        self.text_embed_dim = text_config.hidden_size\n",
    "\n",
    "        text_module = FLAX_MODEL_MAPPING[self.config.text_config.__class__].module_class\n",
    "\n",
    "        self.text_model = text_module(text_config, dtype=self.dtype)\n",
    "\n",
    "        self.text_projection = nn.Dense(\n",
    "            self.projection_dim,\n",
    "            dtype=self.dtype,\n",
    "            kernel_init=jax.nn.initializers.normal(0.02, dtype=self.dtype),\n",
    "            use_bias=False,\n",
    "        )\n",
    "\n",
    "        self.logits = nn.Dense(3) \n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        deterministic: bool = True,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.return_dict\n",
    "\n",
    "        text_outputs = self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            deterministic=deterministic,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        text_embeds = text_outputs[1]\n",
    "        text_embeds = self.text_projection(text_embeds)\n",
    "        logits = self.logits(text_embeds)\n",
    "\n",
    "        return FeedbackJaxOutput(\n",
    "            logits=logits,\n",
    "            text_outputs=text_outputs\n",
    "        )\n",
    "\n",
    "\n",
    "class FeedbackJax(FlaxPreTrainedModel):\n",
    "    config_class = FeedbackJaxConfig\n",
    "    module_class = FeedbackJaxModule\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: FeedbackJaxConfig,\n",
    "        input_shape: Optional[Tuple] = None,\n",
    "        seed: int = 0,\n",
    "        dtype: jnp.dtype = jnp.float32,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if input_shape is None:\n",
    "            input_shape = ((1, 1), (1, 224, 224, 3))\n",
    "        module = self.module_class(config=config, dtype=dtype, **kwargs)\n",
    "        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype)\n",
    "\n",
    "    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple) -> FrozenDict:\n",
    "        # init input tensor\n",
    "        input_ids = jnp.zeros(input_shape[0], dtype=\"i4\")\n",
    "        token_type_ids = jnp.ones_like(input_ids)\n",
    "        attention_mask = jnp.ones_like(input_ids)\n",
    "\n",
    "        params_rng, dropout_rng = jax.random.split(rng)\n",
    "        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n",
    "\n",
    "        return self.module.init(rngs, input_ids, attention_mask, token_type_ids)[\"params\"]\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        params: dict = None,\n",
    "        dropout_rng: jax.random.PRNGKey = None,\n",
    "        train: bool = False,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.return_dict\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = jnp.zeros_like(input_ids)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = jnp.ones_like(input_ids)\n",
    "\n",
    "        # Handle any PRNG if needed\n",
    "        rngs = {}\n",
    "        if dropout_rng is not None:\n",
    "            rngs[\"dropout\"] = dropout_rng\n",
    "\n",
    "        return self.module.apply(\n",
    "            {\"params\": params or self.params},\n",
    "            jnp.array(input_ids, dtype=\"i4\"),\n",
    "            jnp.array(attention_mask, dtype=\"i4\"),\n",
    "            jnp.array(position_ids, dtype=\"i4\"),\n",
    "            jnp.array(token_type_ids, dtype=\"i4\"),\n",
    "            not train,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def get_text_features(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        dropout_rng: jax.random.PRNGKey = None,\n",
    "        train=False,\n",
    "    ):\n",
    "        if position_ids is None:\n",
    "            position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = jnp.zeros_like(input_ids)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = jnp.ones_like(input_ids)\n",
    "\n",
    "        # Handle any PRNG if needed\n",
    "        rngs = {}\n",
    "        if dropout_rng is not None:\n",
    "            rngs[\"dropout\"] = dropout_rng\n",
    "\n",
    "        def _get_features(module, input_ids, attention_mask, position_ids, token_type_ids, deterministic):\n",
    "            text_outputs = module.text_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                deterministic=deterministic,\n",
    "            )\n",
    "\n",
    "            pooled_output = text_outputs[1]\n",
    "            text_features = module.text_projection(pooled_output)\n",
    "            logits = module.logits(text_features)\n",
    "            return logits, text_outputs\n",
    "\n",
    "        return self.module.apply(\n",
    "            {\"params\": self.params},\n",
    "            jnp.array(input_ids, dtype=\"i4\"),\n",
    "            jnp.array(attention_mask, dtype=\"i4\"),\n",
    "            jnp.array(position_ids, dtype=\"i4\"),\n",
    "            jnp.array(token_type_ids, dtype=\"i4\"),\n",
    "            not train,\n",
    "            method=_get_features,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        model_name_or_path: str = None,\n",
    "        *model_args,\n",
    "        **kwargs,\n",
    "    ) -> FlaxPreTrainedModel:\n",
    "\n",
    "        kwargs_text = {\n",
    "            argument[len(\"text_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"text_\")\n",
    "        }\n",
    "\n",
    "        # remove text, vision kwargs from kwargs\n",
    "        for key in kwargs_text.keys():\n",
    "            del kwargs[\"text_\" + key]\n",
    "\n",
    "        # Load and initialize the text and vision model\n",
    "        text_model = kwargs_text.pop(\"model\", None)\n",
    "        if text_model is None:\n",
    "            assert (\n",
    "                model_name_or_path is not None\n",
    "            ), \"If `model` is not defined as an argument, a `text_model_name_or_path` has to be defined\"\n",
    "            from transformers import FlaxAutoModel\n",
    "\n",
    "            if \"config\" not in kwargs_text:\n",
    "                from transformers import AutoConfig\n",
    "\n",
    "                text_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "                kwargs_text[\"config\"] = text_config\n",
    "\n",
    "            text_model = FlaxAutoModel.from_pretrained(model_name_or_path, *model_args, **kwargs_text)\n",
    "\n",
    "\n",
    "        # instantiate config with corresponding kwargs\n",
    "        dtype = kwargs.pop(\"dtype\", jnp.float32)\n",
    "        config = FeedbackJaxConfig.from_text_configs(text_model.config, **kwargs)\n",
    "\n",
    "        # init model\n",
    "        model = cls(config, *model_args, dtype=dtype, **kwargs)\n",
    "\n",
    "        model.params[\"text_model\"] = text_model.params\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/opt-125m were not used when initializing FlaxOPTModel: {('decoder', 'final_layer_norm', 'scale'), ('decoder', 'final_layer_norm', 'bias')}\n",
      "- This IS expected if you are initializing FlaxOPTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxOPTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some of the weights of FlaxOPTModel were initialized in float16 precision from the model checkpoint at facebook/opt-125m:\n",
      "[('decoder', 'embed_positions', 'embedding'), ('decoder', 'embed_tokens', 'embedding'), ('decoder', 'layers', '0', 'fc1', 'bias'), ('decoder', 'layers', '0', 'fc1', 'kernel'), ('decoder', 'layers', '0', 'fc2', 'bias'), ('decoder', 'layers', '0', 'fc2', 'kernel'), ('decoder', 'layers', '0', 'final_layer_norm', 'bias'), ('decoder', 'layers', '0', 'final_layer_norm', 'scale'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '0', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '1', 'fc1', 'bias'), ('decoder', 'layers', '1', 'fc1', 'kernel'), ('decoder', 'layers', '1', 'fc2', 'bias'), ('decoder', 'layers', '1', 'fc2', 'kernel'), ('decoder', 'layers', '1', 'final_layer_norm', 'bias'), ('decoder', 'layers', '1', 'final_layer_norm', 'scale'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '1', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '10', 'fc1', 'bias'), ('decoder', 'layers', '10', 'fc1', 'kernel'), ('decoder', 'layers', '10', 'fc2', 'bias'), ('decoder', 'layers', '10', 'fc2', 'kernel'), ('decoder', 'layers', '10', 'final_layer_norm', 'bias'), ('decoder', 'layers', '10', 'final_layer_norm', 'scale'), ('decoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '10', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '10', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '11', 'fc1', 'bias'), ('decoder', 'layers', '11', 'fc1', 'kernel'), ('decoder', 'layers', '11', 'fc2', 'bias'), ('decoder', 'layers', '11', 'fc2', 'kernel'), ('decoder', 'layers', '11', 'final_layer_norm', 'bias'), ('decoder', 'layers', '11', 'final_layer_norm', 'scale'), ('decoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '11', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '11', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '2', 'fc1', 'bias'), ('decoder', 'layers', '2', 'fc1', 'kernel'), ('decoder', 'layers', '2', 'fc2', 'bias'), ('decoder', 'layers', '2', 'fc2', 'kernel'), ('decoder', 'layers', '2', 'final_layer_norm', 'bias'), ('decoder', 'layers', '2', 'final_layer_norm', 'scale'), ('decoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '2', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '2', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '3', 'fc1', 'bias'), ('decoder', 'layers', '3', 'fc1', 'kernel'), ('decoder', 'layers', '3', 'fc2', 'bias'), ('decoder', 'layers', '3', 'fc2', 'kernel'), ('decoder', 'layers', '3', 'final_layer_norm', 'bias'), ('decoder', 'layers', '3', 'final_layer_norm', 'scale'), ('decoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '3', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '3', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '4', 'fc1', 'bias'), ('decoder', 'layers', '4', 'fc1', 'kernel'), ('decoder', 'layers', '4', 'fc2', 'bias'), ('decoder', 'layers', '4', 'fc2', 'kernel'), ('decoder', 'layers', '4', 'final_layer_norm', 'bias'), ('decoder', 'layers', '4', 'final_layer_norm', 'scale'), ('decoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '4', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '4', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '5', 'fc1', 'bias'), ('decoder', 'layers', '5', 'fc1', 'kernel'), ('decoder', 'layers', '5', 'fc2', 'bias'), ('decoder', 'layers', '5', 'fc2', 'kernel'), ('decoder', 'layers', '5', 'final_layer_norm', 'bias'), ('decoder', 'layers', '5', 'final_layer_norm', 'scale'), ('decoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '5', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '5', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '6', 'fc1', 'bias'), ('decoder', 'layers', '6', 'fc1', 'kernel'), ('decoder', 'layers', '6', 'fc2', 'bias'), ('decoder', 'layers', '6', 'fc2', 'kernel'), ('decoder', 'layers', '6', 'final_layer_norm', 'bias'), ('decoder', 'layers', '6', 'final_layer_norm', 'scale'), ('decoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '6', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '6', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '7', 'fc1', 'bias'), ('decoder', 'layers', '7', 'fc1', 'kernel'), ('decoder', 'layers', '7', 'fc2', 'bias'), ('decoder', 'layers', '7', 'fc2', 'kernel'), ('decoder', 'layers', '7', 'final_layer_norm', 'bias'), ('decoder', 'layers', '7', 'final_layer_norm', 'scale'), ('decoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '7', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '7', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '8', 'fc1', 'bias'), ('decoder', 'layers', '8', 'fc1', 'kernel'), ('decoder', 'layers', '8', 'fc2', 'bias'), ('decoder', 'layers', '8', 'fc2', 'kernel'), ('decoder', 'layers', '8', 'final_layer_norm', 'bias'), ('decoder', 'layers', '8', 'final_layer_norm', 'scale'), ('decoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '8', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '8', 'self_attn_layer_norm', 'scale'), ('decoder', 'layers', '9', 'fc1', 'bias'), ('decoder', 'layers', '9', 'fc1', 'kernel'), ('decoder', 'layers', '9', 'fc2', 'bias'), ('decoder', 'layers', '9', 'fc2', 'kernel'), ('decoder', 'layers', '9', 'final_layer_norm', 'bias'), ('decoder', 'layers', '9', 'final_layer_norm', 'scale'), ('decoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('decoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('decoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('decoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('decoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('decoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('decoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('decoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('decoder', 'layers', '9', 'self_attn_layer_norm', 'bias'), ('decoder', 'layers', '9', 'self_attn_layer_norm', 'scale')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/kaggle/working/notebooks/models.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=1'>2</a>\u001b[0m cfg\u001b[39m.\u001b[39mfrom_pt \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=3'>4</a>\u001b[0m cfg\u001b[39m.\u001b[39mfreeze_backbones \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m FeedbackJax\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=6'>7</a>\u001b[0m     cfg\u001b[39m.\u001b[39;49mmodel_name_or_path,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m     seed\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=8'>9</a>\u001b[0m     dtype\u001b[39m=\u001b[39;49mjnp\u001b[39m.\u001b[39;49mfloat32,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=9'>10</a>\u001b[0m     text_from_pt\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=10'>11</a>\u001b[0m     freeze_backbones\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mfreeze_backbones\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000008vscode-remote?line=12'>13</a>\u001b[0m config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n",
      "\u001b[1;32m/kaggle/working/notebooks/models.ipynb Cell 8'\u001b[0m in \u001b[0;36mFeedbackJax.from_pretrained\u001b[0;34m(cls, model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=268'>269</a>\u001b[0m config \u001b[39m=\u001b[39m FeedbackJaxConfig\u001b[39m.\u001b[39mfrom_text_configs(text_model\u001b[39m.\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=270'>271</a>\u001b[0m \u001b[39m# init model\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=271'>272</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, dtype\u001b[39m=\u001b[39;49mdtype, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=273'>274</a>\u001b[0m model\u001b[39m.\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mtext_model\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m text_model\u001b[39m.\u001b[39mparams\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=275'>276</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[1;32m/kaggle/working/notebooks/models.ipynb Cell 8'\u001b[0m in \u001b[0;36mFeedbackJax.__init__\u001b[0;34m(self, config, input_shape, seed, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=124'>125</a>\u001b[0m     input_shape \u001b[39m=\u001b[39m ((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), (\u001b[39m1\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=125'>126</a>\u001b[0m module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_class(config\u001b[39m=\u001b[39mconfig, dtype\u001b[39m=\u001b[39mdtype, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=126'>127</a>\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config, module, input_shape\u001b[39m=\u001b[39;49minput_shape, seed\u001b[39m=\u001b[39;49mseed, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[0;32m/kaggle/working/transformers/src/transformers/modeling_flax_utils.py:124\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.__init__\u001b[0;34m(self, config, module, input_shape, seed, dtype, _do_init)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_initialized \u001b[39m=\u001b[39m _do_init\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m _do_init:\n\u001b[1;32m    123\u001b[0m     \u001b[39m# randomly initialized parameters\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     random_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_weights(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey, input_shape)\n\u001b[1;32m    125\u001b[0m     params_shape_tree \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39meval_shape(\u001b[39mlambda\u001b[39;00m params: params, random_params)\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m/kaggle/working/notebooks/models.ipynb Cell 8'\u001b[0m in \u001b[0;36mFeedbackJax.init_weights\u001b[0;34m(self, rng, input_shape)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=134'>135</a>\u001b[0m params_rng, dropout_rng \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(rng)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=135'>136</a>\u001b[0m rngs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: params_rng, \u001b[39m\"\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m\"\u001b[39m: dropout_rng}\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=137'>138</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule\u001b[39m.\u001b[39;49minit(rngs, input_ids, attention_mask, token_type_ids)[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "\u001b[1;32m/kaggle/working/notebooks/models.ipynb Cell 8'\u001b[0m in \u001b[0;36mFeedbackJaxModule.__call__\u001b[0;34m(self, input_ids, attention_mask, position_ids, token_type_ids, deterministic, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=77'>78</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=78'>79</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=79'>80</a>\u001b[0m     input_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=86'>87</a>\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=87'>88</a>\u001b[0m ):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=88'>89</a>\u001b[0m     return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mreturn_dict\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=90'>91</a>\u001b[0m     text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=91'>92</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=92'>93</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=93'>94</a>\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=94'>95</a>\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=95'>96</a>\u001b[0m         deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=96'>97</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=97'>98</a>\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=98'>99</a>\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=99'>100</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=101'>102</a>\u001b[0m     text_embeds \u001b[39m=\u001b[39m text_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000007vscode-remote?line=102'>103</a>\u001b[0m     text_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_projection(text_embeds)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/kaggle/working/jax/lib/python3.8/site-packages/flax/linen/module.py:648\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m _context\u001b[39m.\u001b[39mmodule_stack\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m)\n\u001b[1;32m    647\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m   y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    649\u001b[0m   \u001b[39mif\u001b[39;00m _context\u001b[39m.\u001b[39mcapture_stack:\n\u001b[1;32m    650\u001b[0m     filter_fn \u001b[39m=\u001b[39m _context\u001b[39m.\u001b[39mcapture_stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "cfg.dtype = \"int\"\n",
    "cfg.from_pt = False\n",
    "\n",
    "cfg.freeze_backbones = False\n",
    "\n",
    "model = FeedbackJax.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    seed=cfg.seed,\n",
    "    dtype=jnp.float32,\n",
    "    text_from_pt=False,\n",
    "    freeze_backbones=cfg.freeze_backbones\n",
    ")\n",
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels = batch.pop(\"labels\")\n",
    "outs = model(**batch)\n",
    "nn.softmax(outs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../shampoo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed_shampoo import distributed_shampoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "distributed_shampoo() missing 2 required positional arguments: 'learning_rate' and 'block_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/kaggle/working/notebooks/models.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225450552d4a41582d73657276696e67227d/kaggle/working/notebooks/models.ipynb#ch0000015vscode-remote?line=0'>1</a>\u001b[0m distributed_shampoo()\n",
      "\u001b[0;31mTypeError\u001b[0m: distributed_shampoo() missing 2 required positional arguments: 'learning_rate' and 'block_size'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6feded5f862e2f5b544fb63d3bb6c48e49c7ae1c966055a5942297c4c48158e1"
  },
  "kernelspec": {
   "display_name": "jax_notebook",
   "language": "python",
   "name": "jax_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
